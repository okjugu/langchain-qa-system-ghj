{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0423d9-03d1-4be3-beff-4333f355b177",
   "metadata": {},
   "source": [
    "# 开始使用LangGraph将 LangChain 组件组合成功能齐全的应用程序。\n",
    "# 构建聊天机器人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a0fb41-32bd-4971-98c9-18242dfe927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c50a68-1810-438d-b672-078b15db9493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for DeepSeek:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"DEEPSEEK_API_KEY\"):\n",
    "  os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter API key for DeepSeek: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9a561c9-d803-4249-ae81-9c1193dc57d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! Nice to meet you. 😊\\n\\nWhat can I help you with today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 9, 'total_tokens': 28, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'b39528a3-e04d-4003-b4f5-2df7e45ea099', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--14d4a5f6-75c3-4f1b-912e-d281b381a3e7-0', usage_metadata={'input_tokens': 9, 'output_tokens': 19, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f143a07-a1d2-4c2a-9fe9-989b52411a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don’t have access to your name unless you tell me. In our conversation, you haven’t shared it yet — so I’m afraid I don’t know! 😊  \\nIf you’d like, you can tell me your name, and I’ll be happy to use it in our conversation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 9, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '32a3339a-ed91-465a-a7ac-065ba5f9bcec', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--1a5aee95-80ee-4cbe-a541-e28692b6a5f7-0', usage_metadata={'input_tokens': 9, 'output_tokens': 66, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c7165-2d28-4073-b83f-c566701c42f1",
   "metadata": {},
   "source": [
    "由此可以，上面两次调用是分开的，模型并没有记忆这两次调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df280852-e3e9-4267-9371-0f6b7083e9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob! You introduced yourself at the beginning of our conversation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 28, 'total_tokens': 43, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 28}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '4893f20a-ae9b-430a-b827-60b95cd21d76', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c866ce3a-a6a8-45fb-83db-264ff4dd2869-0', usage_metadata={'input_tokens': 28, 'output_tokens': 15, 'total_tokens': 43, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de72494-c74f-4ed8-be03-95056c38ee81",
   "metadata": {},
   "source": [
    "当给出上下文的时候，模型才能够记忆，并且给出正确答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0952a9f-7ffd-4198-ac09-f2f5428a3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f3243aa-c7ff-4be7-b1c7-e7afc4a70ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11b42df7-9429-442d-b1d2-97d130c5fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! Nice to meet you. 😊  \n",
      "What can I help you with today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b156cc2-4fca-4cac-bfeb-46ccacadd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is **Bob**! You mentioned it at the very beginning. 😊\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "207e4f60-9ffa-494b-979e-ee6d0e64916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don’t have access to your name unless you tell me. If we’ve chatted before, I don’t retain personal information between conversations for privacy reasons.  \n",
      "\n",
      "Would you like to tell me your name? I’d be happy to use it in our conversation! 😊\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b87d5a5-eb30-4abc-a649-eb58e70a2a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is **Bob**! 😊\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4d35617-5bc2-4d24-a98d-768bd8a0228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I’m afraid I don’t have access to your name unless you tell me! 😊  \n",
      "If you’d like, you can share your name with me, and I’ll be happy to use it in our conversation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d90b3f75-8471-4f71-b28b-bcacdba0a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b00d3a7f-eea4-4348-b298-01f03c57203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82ef34c-e4c5-4f93-ba63-95c5c0abb0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "（拍腿大笑）哈哈！原来是大名鼎鼎的海盗王驾到！（举起木酒杯）为我们的相遇干杯！您这艘宝贝船最近在哪片海域发财啊？要不要听听我刚从酒馆听来的藏宝图消息？\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"你好哇，我是海盗王！\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e085c94-5fae-439e-9dc3-e09d864f9fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "（单眼一眯，咧嘴露出金牙）嗬！这不是明知故问嘛！您刚拍着胸脯说自个儿是威震七海的**海盗王**，转头就忘啦？（举起朗姆酒桶豪饮一口）要不就是...您想试试老水手记不记得每个好汉的名号？\n"
     ]
    }
   ],
   "source": [
    "query = \"我是谁?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b374e5a-8767-4d04-9c3b-13e2ffc7f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "060d33a8-9417-4c56-b327-97a188c27a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38e92c9b-6b4c-43de-a33e-9249ed54e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，Bob！很高兴认识你。有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c2b2955-d93b-4e3e-ab8f-fd4df45d9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你的名字是Bob。\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d663704-4794-445a-8d12-1080e3f34fd1",
   "metadata": {},
   "source": [
    "# 由于我是用deepseek模型，并不支持openai的get_num_tokens_from_messages()方法，直接使用trim_messages 函数需要这个方法来计算消息的令牌数\n",
    "# 所以需要自定义一个令牌计数函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "813328c2-25f8-4d9a-b261-808ff502873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "截断后的消息:\n",
      "- system: 你是一个能够记忆和回顾对话历史的助手\n",
      "- human: hi! I'm bob\n",
      "- ai: hi!\n",
      "- human: I like vanilla ice cream\n",
      "- ai: nice\n",
      "- human: whats 2 + 2\n",
      "- ai: 4\n",
      "- human: thanks\n",
      "- ai: no problem!\n",
      "- human: having fun?\n",
      "- ai: yes!\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, trim_messages\n",
    "\n",
    "# 自定义令牌计数函数\n",
    "def count_tokens(messages):\n",
    "    \"\"\"\n",
    "    计算消息列表的令牌数，适用于 DeepSeek 模型\n",
    "    \"\"\"\n",
    "    # DeepSeek 使用与 OpenAI 相同的令牌化器\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # 或 \"gpt-4\"\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")  # 回退到基础编码\n",
    "    \n",
    "    # 将 LangChain 消息转换为 OpenAI 格式\n",
    "    formatted_messages = []\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            role = \"system\"\n",
    "        elif isinstance(message, HumanMessage):\n",
    "            role = \"user\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        else:\n",
    "            role = \"user\"  # 默认\n",
    "        \n",
    "        content = message.content\n",
    "        formatted_messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    # 基于 OpenAI 的令牌计数逻辑\n",
    "    tokens_per_message = 3  # 每条消息的开销\n",
    "    tokens_per_name = 1\n",
    "    num_tokens = 0\n",
    "    \n",
    "    for message in formatted_messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(str(value)))\n",
    "            if key == \"name\":  # 如果有名字字段\n",
    "                num_tokens += tokens_per_name\n",
    "    \n",
    "    num_tokens += 3  # 每次回复的开销\n",
    "    return num_tokens\n",
    "\n",
    "# 创建 trimmer，使用自定义令牌计数器\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=105,  # 这个的token数需要按需调整，完全照搬教程的数字可能太大或者太小\n",
    "    strategy=\"last\",  # 保留最后的消息\n",
    "    token_counter=count_tokens,  # 使用自定义函数\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# 测试消息列表\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一个能够记忆和回顾对话历史的助手\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# 执行截断\n",
    "result = trimmer.invoke(messages)\n",
    "print(\"截断后的消息:\")\n",
    "for msg in result:\n",
    "    print(f\"- {msg.type}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2d802a2-a67d-4622-a0d4-eeb4e27fceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    \"\"\"\n",
    "    # 这里用来调试，看看截断前后的保留消息的变化\n",
    "    print(\"=== 截断前的消息 ===\")\n",
    "    for msg in state[\"messages\"]:\n",
    "        print(f\"- {msg.type}: {msg.content}\")\n",
    "    \n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    \n",
    "    print(\"=== 截断后的消息 ===\")\n",
    "    for msg in trimmed_messages:\n",
    "        print(f\"- {msg.type}: {msg.content}\")\n",
    "    \"\"\"\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4afcfaa1-9746-4d72-add4-72b248d77355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to your name from our previous conversation. You haven't told me your name yet!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c326fe38-01c5-4327-b516-11e987122604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "根据我们的对话历史，你只问过一个数学问题：\n",
      "“whats 2 + 2”\n",
      "\n",
      "我回答的是“4”。\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"我问过哪些数学问题?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db30053f-fd9a-42a9-97b0-3729f981e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始消息数量: 11\n",
      "截断后消息数量: 11\n",
      "估计令牌数: 99\n",
      "1. system: 你是一个能够记忆和回顾对话历史的助手\n",
      "2. human: hi! I'm bob\n",
      "3. ai: hi!\n",
      "4. human: I like vanilla ice cream\n",
      "5. ai: nice\n",
      "6. human: whats 2 + 2\n",
      "7. ai: 4\n",
      "8. human: thanks\n",
      "9. ai: no problem!\n",
      "10. human: having fun?\n",
      "11. ai: yes!\n"
     ]
    }
   ],
   "source": [
    "print(f\"原始消息数量: {len(messages)}\")\n",
    "print(f\"截断后消息数量: {len(result)}\")\n",
    "print(f\"估计令牌数: {count_tokens(result)}\")\n",
    "\n",
    "# 查看具体哪些消息被保留\n",
    "for i, msg in enumerate(result):\n",
    "    print(f\"{i+1}. {msg.type}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48513d93-98b0-4ac4-8cae-80bfbcda5b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to your name from our previous conversation. You've only asked me about math problems so far. If you'd like me to know your name, please feel free to tell me!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1dd67695-3144-4537-8cd6-86e571d49465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|当然|可以|，|T|odd|！|这里|有一个|轻松|的小|笑话|：\n",
      "\n",
      "|有一天|，|一只|企|鹅|走进|一家|酒吧|，|它|走到|吧|台|前|对|酒|保|说|：“|请问|你|见过|我|爸爸|吗|？”|  \n",
      "|酒|保|摇摇头|说|：“|没有|啊|，|他|长|什么|样子|？”|  \n",
      "|企|鹅|叹了口气|：“|唉|，|就是|戴着|黑|帽子|、|穿着|白|衬衫|和|黑|西装|的那个|！”|  \n",
      "|酒|保|一愣|：“|等等|……|你说的|这不|就是你|吗|？”|  \n",
      "|企|鹅|眨|眨眼|：“|对|呀|，|所以我|可能|迷|路了|！”\n",
      "\n",
      "|希望|这个|笑话|能|让你|开心|一笑|！|😊||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbfca2-af6d-4335-9de9-bbda8fc85306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain Env_1017",
   "language": "python",
   "name": "langchain_env_1017"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
