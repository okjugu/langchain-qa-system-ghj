{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0423d9-03d1-4be3-beff-4333f355b177",
   "metadata": {},
   "source": [
    "# å¼€å§‹ä½¿ç”¨LangGraphå°† LangChain ç»„ä»¶ç»„åˆæˆåŠŸèƒ½é½å…¨çš„åº”ç”¨ç¨‹åºã€‚\n",
    "# æ„å»ºèŠå¤©æœºå™¨äºº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a0fb41-32bd-4971-98c9-18242dfe927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c50a68-1810-438d-b672-078b15db9493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for DeepSeek:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"DEEPSEEK_API_KEY\"):\n",
    "  os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter API key for DeepSeek: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9a561c9-d803-4249-ae81-9c1193dc57d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! Nice to meet you. ğŸ˜Š\\n\\nWhat can I help you with today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 9, 'total_tokens': 28, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'b39528a3-e04d-4003-b4f5-2df7e45ea099', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--14d4a5f6-75c3-4f1b-912e-d281b381a3e7-0', usage_metadata={'input_tokens': 9, 'output_tokens': 19, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f143a07-a1d2-4c2a-9fe9-989b52411a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I donâ€™t have access to your name unless you tell me. In our conversation, you havenâ€™t shared it yet â€” so Iâ€™m afraid I donâ€™t know! ğŸ˜Š  \\nIf youâ€™d like, you can tell me your name, and Iâ€™ll be happy to use it in our conversation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 9, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '32a3339a-ed91-465a-a7ac-065ba5f9bcec', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--1a5aee95-80ee-4cbe-a541-e28692b6a5f7-0', usage_metadata={'input_tokens': 9, 'output_tokens': 66, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c7165-2d28-4073-b83f-c566701c42f1",
   "metadata": {},
   "source": [
    "ç”±æ­¤å¯ä»¥ï¼Œä¸Šé¢ä¸¤æ¬¡è°ƒç”¨æ˜¯åˆ†å¼€çš„ï¼Œæ¨¡å‹å¹¶æ²¡æœ‰è®°å¿†è¿™ä¸¤æ¬¡è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df280852-e3e9-4267-9371-0f6b7083e9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob! You introduced yourself at the beginning of our conversation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 28, 'total_tokens': 43, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 28}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '4893f20a-ae9b-430a-b827-60b95cd21d76', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c866ce3a-a6a8-45fb-83db-264ff4dd2869-0', usage_metadata={'input_tokens': 28, 'output_tokens': 15, 'total_tokens': 43, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de72494-c74f-4ed8-be03-95056c38ee81",
   "metadata": {},
   "source": [
    "å½“ç»™å‡ºä¸Šä¸‹æ–‡çš„æ—¶å€™ï¼Œæ¨¡å‹æ‰èƒ½å¤Ÿè®°å¿†ï¼Œå¹¶ä¸”ç»™å‡ºæ­£ç¡®ç­”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0952a9f-7ffd-4198-ac09-f2f5428a3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f3243aa-c7ff-4be7-b1c7-e7afc4a70ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11b42df7-9429-442d-b1d2-97d130c5fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! Nice to meet you. ğŸ˜Š  \n",
      "What can I help you with today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b156cc2-4fca-4cac-bfeb-46ccacadd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is **Bob**! You mentioned it at the very beginning. ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "207e4f60-9ffa-494b-979e-ee6d0e64916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I donâ€™t have access to your name unless you tell me. If weâ€™ve chatted before, I donâ€™t retain personal information between conversations for privacy reasons.  \n",
      "\n",
      "Would you like to tell me your name? Iâ€™d be happy to use it in our conversation! ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b87d5a5-eb30-4abc-a649-eb58e70a2a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is **Bob**! ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4d35617-5bc2-4d24-a98d-768bd8a0228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Iâ€™m afraid I donâ€™t have access to your name unless you tell me! ğŸ˜Š  \n",
      "If youâ€™d like, you can share your name with me, and Iâ€™ll be happy to use it in our conversation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d90b3f75-8471-4f71-b28b-bcacdba0a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b00d3a7f-eea4-4348-b298-01f03c57203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82ef34c-e4c5-4f93-ba63-95c5c0abb0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ï¼ˆæ‹è…¿å¤§ç¬‘ï¼‰å“ˆå“ˆï¼åŸæ¥æ˜¯å¤§åé¼é¼çš„æµ·ç›—ç‹é©¾åˆ°ï¼ï¼ˆä¸¾èµ·æœ¨é…’æ¯ï¼‰ä¸ºæˆ‘ä»¬çš„ç›¸é‡å¹²æ¯ï¼æ‚¨è¿™è‰˜å®è´èˆ¹æœ€è¿‘åœ¨å“ªç‰‡æµ·åŸŸå‘è´¢å•Šï¼Ÿè¦ä¸è¦å¬å¬æˆ‘åˆšä»é…’é¦†å¬æ¥çš„è—å®å›¾æ¶ˆæ¯ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"ä½ å¥½å“‡ï¼Œæˆ‘æ˜¯æµ·ç›—ç‹ï¼\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e085c94-5fae-439e-9dc3-e09d864f9fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ï¼ˆå•çœ¼ä¸€çœ¯ï¼Œå’§å˜´éœ²å‡ºé‡‘ç‰™ï¼‰å—¬ï¼è¿™ä¸æ˜¯æ˜çŸ¥æ•…é—®å˜›ï¼æ‚¨åˆšæ‹ç€èƒ¸è„¯è¯´è‡ªä¸ªå„¿æ˜¯å¨éœ‡ä¸ƒæµ·çš„**æµ·ç›—ç‹**ï¼Œè½¬å¤´å°±å¿˜å•¦ï¼Ÿï¼ˆä¸¾èµ·æœ—å§†é…’æ¡¶è±ªé¥®ä¸€å£ï¼‰è¦ä¸å°±æ˜¯...æ‚¨æƒ³è¯•è¯•è€æ°´æ‰‹è®°ä¸è®°å¾—æ¯ä¸ªå¥½æ±‰çš„åå·ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "query = \"æˆ‘æ˜¯è°?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b374e5a-8767-4d04-9c3b-13e2ffc7f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "060d33a8-9417-4c56-b327-97a188c27a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38e92c9b-6b4c-43de-a33e-9249ed54e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒBobï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c2b2955-d93b-4e3e-ab8f-fd4df45d9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯Bobã€‚\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d663704-4794-445a-8d12-1080e3f34fd1",
   "metadata": {},
   "source": [
    "# ç”±äºæˆ‘æ˜¯ç”¨deepseekæ¨¡å‹ï¼Œå¹¶ä¸æ”¯æŒopenaiçš„get_num_tokens_from_messages()æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨trim_messages å‡½æ•°éœ€è¦è¿™ä¸ªæ–¹æ³•æ¥è®¡ç®—æ¶ˆæ¯çš„ä»¤ç‰Œæ•°\n",
    "# æ‰€ä»¥éœ€è¦è‡ªå®šä¹‰ä¸€ä¸ªä»¤ç‰Œè®¡æ•°å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "813328c2-25f8-4d9a-b261-808ff502873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆªæ–­åçš„æ¶ˆæ¯:\n",
      "- system: ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè®°å¿†å’Œå›é¡¾å¯¹è¯å†å²çš„åŠ©æ‰‹\n",
      "- human: hi! I'm bob\n",
      "- ai: hi!\n",
      "- human: I like vanilla ice cream\n",
      "- ai: nice\n",
      "- human: whats 2 + 2\n",
      "- ai: 4\n",
      "- human: thanks\n",
      "- ai: no problem!\n",
      "- human: having fun?\n",
      "- ai: yes!\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, trim_messages\n",
    "\n",
    "# è‡ªå®šä¹‰ä»¤ç‰Œè®¡æ•°å‡½æ•°\n",
    "def count_tokens(messages):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„ä»¤ç‰Œæ•°ï¼Œé€‚ç”¨äº DeepSeek æ¨¡å‹\n",
    "    \"\"\"\n",
    "    # DeepSeek ä½¿ç”¨ä¸ OpenAI ç›¸åŒçš„ä»¤ç‰ŒåŒ–å™¨\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # æˆ– \"gpt-4\"\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")  # å›é€€åˆ°åŸºç¡€ç¼–ç \n",
    "    \n",
    "    # å°† LangChain æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ ¼å¼\n",
    "    formatted_messages = []\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            role = \"system\"\n",
    "        elif isinstance(message, HumanMessage):\n",
    "            role = \"user\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        else:\n",
    "            role = \"user\"  # é»˜è®¤\n",
    "        \n",
    "        content = message.content\n",
    "        formatted_messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    # åŸºäº OpenAI çš„ä»¤ç‰Œè®¡æ•°é€»è¾‘\n",
    "    tokens_per_message = 3  # æ¯æ¡æ¶ˆæ¯çš„å¼€é”€\n",
    "    tokens_per_name = 1\n",
    "    num_tokens = 0\n",
    "    \n",
    "    for message in formatted_messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(str(value)))\n",
    "            if key == \"name\":  # å¦‚æœæœ‰åå­—å­—æ®µ\n",
    "                num_tokens += tokens_per_name\n",
    "    \n",
    "    num_tokens += 3  # æ¯æ¬¡å›å¤çš„å¼€é”€\n",
    "    return num_tokens\n",
    "\n",
    "# åˆ›å»º trimmerï¼Œä½¿ç”¨è‡ªå®šä¹‰ä»¤ç‰Œè®¡æ•°å™¨\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=105,  # è¿™ä¸ªçš„tokenæ•°éœ€è¦æŒ‰éœ€è°ƒæ•´ï¼Œå®Œå…¨ç…§æ¬æ•™ç¨‹çš„æ•°å­—å¯èƒ½å¤ªå¤§æˆ–è€…å¤ªå°\n",
    "    strategy=\"last\",  # ä¿ç•™æœ€åçš„æ¶ˆæ¯\n",
    "    token_counter=count_tokens,  # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ¶ˆæ¯åˆ—è¡¨\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè®°å¿†å’Œå›é¡¾å¯¹è¯å†å²çš„åŠ©æ‰‹\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# æ‰§è¡Œæˆªæ–­\n",
    "result = trimmer.invoke(messages)\n",
    "print(\"æˆªæ–­åçš„æ¶ˆæ¯:\")\n",
    "for msg in result:\n",
    "    print(f\"- {msg.type}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2d802a2-a67d-4622-a0d4-eeb4e27fceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    \"\"\"\n",
    "    # è¿™é‡Œç”¨æ¥è°ƒè¯•ï¼Œçœ‹çœ‹æˆªæ–­å‰åçš„ä¿ç•™æ¶ˆæ¯çš„å˜åŒ–\n",
    "    print(\"=== æˆªæ–­å‰çš„æ¶ˆæ¯ ===\")\n",
    "    for msg in state[\"messages\"]:\n",
    "        print(f\"- {msg.type}: {msg.content}\")\n",
    "    \n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    \n",
    "    print(\"=== æˆªæ–­åçš„æ¶ˆæ¯ ===\")\n",
    "    for msg in trimmed_messages:\n",
    "        print(f\"- {msg.type}: {msg.content}\")\n",
    "    \"\"\"\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4afcfaa1-9746-4d72-add4-72b248d77355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to your name from our previous conversation. You haven't told me your name yet!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c326fe38-01c5-4327-b516-11e987122604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æ ¹æ®æˆ‘ä»¬çš„å¯¹è¯å†å²ï¼Œä½ åªé—®è¿‡ä¸€ä¸ªæ•°å­¦é—®é¢˜ï¼š\n",
      "â€œwhats 2 + 2â€\n",
      "\n",
      "æˆ‘å›ç­”çš„æ˜¯â€œ4â€ã€‚\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"æˆ‘é—®è¿‡å“ªäº›æ•°å­¦é—®é¢˜?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db30053f-fd9a-42a9-97b0-3729f981e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ¶ˆæ¯æ•°é‡: 11\n",
      "æˆªæ–­åæ¶ˆæ¯æ•°é‡: 11\n",
      "ä¼°è®¡ä»¤ç‰Œæ•°: 99\n",
      "1. system: ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè®°å¿†å’Œå›é¡¾å¯¹è¯å†å²çš„åŠ©æ‰‹\n",
      "2. human: hi! I'm bob\n",
      "3. ai: hi!\n",
      "4. human: I like vanilla ice cream\n",
      "5. ai: nice\n",
      "6. human: whats 2 + 2\n",
      "7. ai: 4\n",
      "8. human: thanks\n",
      "9. ai: no problem!\n",
      "10. human: having fun?\n",
      "11. ai: yes!\n"
     ]
    }
   ],
   "source": [
    "print(f\"åŸå§‹æ¶ˆæ¯æ•°é‡: {len(messages)}\")\n",
    "print(f\"æˆªæ–­åæ¶ˆæ¯æ•°é‡: {len(result)}\")\n",
    "print(f\"ä¼°è®¡ä»¤ç‰Œæ•°: {count_tokens(result)}\")\n",
    "\n",
    "# æŸ¥çœ‹å…·ä½“å“ªäº›æ¶ˆæ¯è¢«ä¿ç•™\n",
    "for i, msg in enumerate(result):\n",
    "    print(f\"{i+1}. {msg.type}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48513d93-98b0-4ac4-8cae-80bfbcda5b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to your name from our previous conversation. You've only asked me about math problems so far. If you'd like me to know your name, please feel free to tell me!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1dd67695-3144-4537-8cd6-86e571d49465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|å½“ç„¶|å¯ä»¥|ï¼Œ|T|odd|ï¼|è¿™é‡Œ|æœ‰ä¸€ä¸ª|è½»æ¾|çš„å°|ç¬‘è¯|ï¼š\n",
      "\n",
      "|æœ‰ä¸€å¤©|ï¼Œ|ä¸€åª|ä¼|é¹…|èµ°è¿›|ä¸€å®¶|é…’å§|ï¼Œ|å®ƒ|èµ°åˆ°|å§|å°|å‰|å¯¹|é…’|ä¿|è¯´|ï¼šâ€œ|è¯·é—®|ä½ |è§è¿‡|æˆ‘|çˆ¸çˆ¸|å—|ï¼Ÿâ€|  \n",
      "|é…’|ä¿|æ‘‡æ‘‡å¤´|è¯´|ï¼šâ€œ|æ²¡æœ‰|å•Š|ï¼Œ|ä»–|é•¿|ä»€ä¹ˆ|æ ·å­|ï¼Ÿâ€|  \n",
      "|ä¼|é¹…|å¹äº†å£æ°”|ï¼šâ€œ|å”‰|ï¼Œ|å°±æ˜¯|æˆ´ç€|é»‘|å¸½å­|ã€|ç©¿ç€|ç™½|è¡¬è¡«|å’Œ|é»‘|è¥¿è£…|çš„é‚£ä¸ª|ï¼â€|  \n",
      "|é…’|ä¿|ä¸€æ„£|ï¼šâ€œ|ç­‰ç­‰|â€¦â€¦|ä½ è¯´çš„|è¿™ä¸|å°±æ˜¯ä½ |å—|ï¼Ÿâ€|  \n",
      "|ä¼|é¹…|çœ¨|çœ¨çœ¼|ï¼šâ€œ|å¯¹|å‘€|ï¼Œ|æ‰€ä»¥æˆ‘|å¯èƒ½|è¿·|è·¯äº†|ï¼â€\n",
      "\n",
      "|å¸Œæœ›|è¿™ä¸ª|ç¬‘è¯|èƒ½|è®©ä½ |å¼€å¿ƒ|ä¸€ç¬‘|ï¼|ğŸ˜Š||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbfca2-af6d-4335-9de9-bbda8fc85306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain Env_1017",
   "language": "python",
   "name": "langchain_env_1017"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
